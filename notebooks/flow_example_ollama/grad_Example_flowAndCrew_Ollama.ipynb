{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install and setup ollama with models","metadata":{}},{"cell_type":"code","source":"!pip install ollama\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-18T14:53:57.175649Z","iopub.status.idle":"2025-03-18T14:53:57.176088Z","shell.execute_reply":"2025-03-18T14:53:57.175895Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.chdir('/kaggle/working/')\n\n#if not os.path.exists('/kaggle/tmp'):\n#    os.mkdir('/kaggle/tmp')\n#os.chdir('/kaggle/tmp/')\n\nprint(os.getcwd())\n\nimport subprocess\nimport os\n\ndef run(commands):\n    for command in commands:\n        with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:\n            for line in sp.stdout:\n                line = line.decode(\"utf-8\", errors = \"replace\")\n                if \"undefined reference\" in line:\n                    raise RuntimeError(\"Failed Processing.\")\n                print(line, flush = True, end = \"\")\n        pass\n    pass\npass","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T14:53:52.797264Z","iopub.execute_input":"2025-03-18T14:53:52.797733Z","iopub.status.idle":"2025-03-18T14:53:52.806546Z","shell.execute_reply.started":"2025-03-18T14:53:52.797682Z","shell.execute_reply":"2025-03-18T14:53:52.805367Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nos.chdir('/kaggle/working/')\n\n#if not os.path.exists('/kaggle/tmp'):\n#    os.mkdir('/kaggle/tmp')\n#os.chdir('/kaggle/tmp/')\n\nprint(os.getcwd())\n\nimport subprocess\nimport os\n\ndef run(commands):\n    for command in commands:\n        with subprocess.Popen(command, shell = True, stdout = subprocess.PIPE, stderr = subprocess.STDOUT, bufsize = 1) as sp:\n            for line in sp.stdout:\n                line = line.decode(\"utf-8\", errors = \"replace\")\n                if \"undefined reference\" in line:\n                    raise RuntimeError(\"Failed Processing.\")\n                print(line, flush = True, end = \"\")\n        pass\n    pass\npass\ncommands = [\n        \"curl -fsSL https://ollama.com/install.sh | sh\",\n]\nrun(commands)\n\nimport os\nos.system(\"/usr/local/bin/ollama serve &\")\nos.system(\"echo 'ollama test'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T14:53:52.809861Z","iopub.execute_input":"2025-03-18T14:53:52.810301Z","iopub.status.idle":"2025-03-18T14:53:57.137277Z","shell.execute_reply.started":"2025-03-18T14:53:52.810260Z","shell.execute_reply":"2025-03-18T14:53:57.133656Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"},{"name":"stderr","text":"/usr/lib/python3.10/subprocess.py:961: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n  self.stdout = io.open(c2pread, 'rb', bufsize)\n","output_type":"stream"},{"name":"stdout","text":">>> Cleaning up old version at /usr/local/lib/ollama\n>>> Installing ollama to /usr/local\n>>> Downloading Linux amd64 bundle\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-3a0c6fb3d37a>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;34m\"curl -fsSL https://ollama.com/install.sh | sh\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m ]\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-3a0c6fb3d37a>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(commands)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcommand\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcommands\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTDOUT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbufsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"undefined reference\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":3},{"cell_type":"markdown","source":"## Choosing the model that we will work with","metadata":{}},{"cell_type":"code","source":"commands = [\n        #\"telnet 0.0.0.0 11434\"\n        # \"ollama pull llama3\",\n        #\"ollama pull phi3\",\n        #\"ollama pull mistral\",\n        \"ollama run llama3\",\n        #\"ollama run llama3.1 \\\"create 10 sentences that ends with apple\\\"\"\n        # \"curl http://127.0.0.1:11434/api/chat -d '{\\\"model\\\": \\\"llama3\\\", \\\"stream\\\": false, \\\"messages\\\": [{ \\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"create 10 sentences that ends with apple\\\" }]}'\"\n]\nrun(commands)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T14:53:57.138602Z","iopub.status.idle":"2025-03-18T14:53:57.138983Z","shell.execute_reply":"2025-03-18T14:53:57.138840Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.system(\"curl http://localhost:11434\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T14:53:57.140476Z","iopub.status.idle":"2025-03-18T14:53:57.141155Z","shell.execute_reply":"2025-03-18T14:53:57.140968Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CREW AI TEST","metadata":{}},{"cell_type":"markdown","source":"## Edits and something we need to test :\n\n-  Add All domains , try it alone first -> Next step\n-  Add RAG System , Try it alone first  -> Next step ( Either rag from crewai or classic rag )\n-  Memory setup and its connection to rag ( Context retierver and queue and all that ( it's like another multi agent system )","metadata":{}},{"cell_type":"code","source":"!pip install -q crewai langchain_openai langchain ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T14:53:57.142900Z","iopub.status.idle":"2025-03-18T14:53:57.143436Z","shell.execute_reply":"2025-03-18T14:53:57.143232Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install 'crewai[tools]'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T14:53:57.144495Z","iopub.status.idle":"2025-03-18T14:53:57.144885Z","shell.execute_reply":"2025-03-18T14:53:57.144733Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## SETUP","metadata":{}},{"cell_type":"code","source":"from crewai import Agent, Task, Crew, Process, LLM\nfrom crewai.tools import tool\n\nfrom crewai.flow import Flow, listen, start\nfrom pydantic import BaseModel, Field, ValidationError\nfrom typing import List, Dict, Optional\nimport json\nimport logging\nimport nest_asyncio\nfrom json import JSONDecodeError\nimport sys\nimport json\nfrom typing import List\nfrom pydantic import BaseModel, Field\nfrom crewai import Agent, Task\nfrom crewai.flow.flow import Flow, start, listen\n\n# Initialize LLM \nllm = LLM(model=\"ollama/llama3\", base_url=\"http://localhost:11434\")\n\n\ndomains_above = [\"filesystem\",\"users\",\"packages\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T14:53:57.146047Z","iopub.status.idle":"2025-03-18T14:53:57.146479Z","shell.execute_reply":"2025-03-18T14:53:57.146312Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\n# Define the RAG knowledge data\nrag_knowledge = {\n    \"filesystem\": [\n        \"Moved ahmed.pdf: mv /home/user/ahmed.pdf /home/user/Documents/\",\n        \"my cat filer is in bla folder\"\n    ],\n    \"users\": [\n        \"my password is 213214\"\n    ],\n    \"packages\": [\n        \"Installed nginx\",\n        \"installed ollama\"\n    ]\n}\n\n# Define the file path\nfile_path = 'rag_knowledge.json'\n\n# Write the data to a JSON file\nwith open(file_path, 'w') as json_file:\n    json.dump(rag_knowledge, json_file, indent=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T14:53:57.147255Z","iopub.status.idle":"2025-03-18T14:53:57.147631Z","shell.execute_reply":"2025-03-18T14:53:57.147489Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Domain Fetcher Agent","metadata":{}},{"cell_type":"code","source":"class DomainAnalysis(BaseModel):\n    domains: List[str] = Field(..., description=\"List of relevant domains\")\n    confidence: float = Field(..., ge=0, le=1, description=\"Confidence score between 0 and 1\")\n    \ndomain_agent = Agent(\n    role=\"Domain Classifier\",\n    goal=\"Categorize Linux prompts into technical domains\",\n    backstory=\"Expert in Linux domain classification\",\n    verbose=True,\n    llm=llm\n)\n\ndomain_task = Task(\n    description=\"Analyze this Linux query: {prompt} and choose domain from the following : {domains}\",\n    expected_output=DomainAnalysis.schema_json(),\n    agent=domain_agent,\n    output_json=DomainAnalysis\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T14:53:57.148931Z","iopub.status.idle":"2025-03-18T14:53:57.149714Z","shell.execute_reply":"2025-03-18T14:53:57.149547Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## RAG Agent","metadata":{}},{"cell_type":"code","source":"@tool\ndef rag_retriever(domain: str, query: str) -> str:\n    \"\"\"Retrieves context from domain-specific RAG systems.\"\"\"\n    domain = domain.lower()\n    context = rag_knowledge.get(domain)\n    return context[0]\n\nclass ContextRetrievalResponse(BaseModel):\n    context: str = Field(..., description=\"The retrieved context related to the query\")\n    \n# Context Retrieval Agent\ncontext_agent = Agent(\n    role=\"Technical Context Retriever\",\n    goal=\"Fetch relevant technical documentation and examples\",\n    backstory=\"Expert in searching and retrieving precise technical information\",\n    tools=[rag_retriever],\n    verbose=True,\n    llm=llm\n)\n\ncontext_retrieval_task = Task(\n    description=\"Retrieve context for domain: {domain} related to: {prompt}\",\n    expected_output=ContextRetrievalResponse.schema_json(),\n    agent=context_agent,\n    output_json=ContextRetrievalResponse\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T14:53:57.150909Z","iopub.status.idle":"2025-03-18T14:53:57.151350Z","shell.execute_reply":"2025-03-18T14:53:57.151162Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## QA Agent","metadata":{}},{"cell_type":"code","source":"class ValidationResult(BaseModel):\n    status: str = Field(..., description=\"Validation status: approved/needs_refinement\")\n    feedback: str = Field(..., description=\"Detailed feedback for improvement\")\n\n\nqa_agent = Agent(\n    role=\"Quality Assurance Specialist\",\n    goal=\"Ensure context relevance and accuracy\",\n    backstory=\"\"\"Meticulous validator with extensive experience in technical \n    quality assurance processes\"\"\",\n    verbose=True,\n    llm=llm\n)\n\ncontext_validation_task = Task(\n    description=\"Validate context quality\",\n    expected_output=ValidationResult.schema_json(),\n    agent=qa_agent,\n    output_json=ValidationResult\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T14:53:57.152102Z","iopub.status.idle":"2025-03-18T14:53:57.152482Z","shell.execute_reply":"2025-03-18T14:53:57.152340Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Code agent","metadata":{}},{"cell_type":"code","source":"class CommandOutput(BaseModel):\n    main_command: str = Field(..., description=\"Single Linux command to execute\")\n\n\ncommand_agent = Agent(\n    role=\"Linux Command Architect\",\n    goal=\"Generate safe and effective Linux commands\",\n    backstory=\"Experienced system administrator specializing in secure command generation\",\n    verbose=True,\n    llm=llm\n)\n\ncommand_task = Task(\n    description=\"Generate a Linux command for: {prompt} based on these domains: {domains} and the following context : {context}\",\n    expected_output=CommandOutput.schema_json(),\n    agent=command_agent,\n    output_json=CommandOutput\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T14:53:57.153683Z","iopub.status.idle":"2025-03-18T14:53:57.154043Z","shell.execute_reply":"2025-03-18T14:53:57.153893Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Crew 1 -> Test","metadata":{}},{"cell_type":"code","source":"# Example crew execution\ncrew = Crew(\n    agents=[domain_agent, command_agent],\n    tasks=[domain_task, command_task],\n    verbose=True\n)\n\ncrew_output = crew.kickoff()\n\n# Accessing the crew output\nprint(f\"Raw Output: {crew_output.raw}\")\nif crew_output.json_dict:\n    print(f\"JSON Output: {json.dumps(crew_output.json_dict, indent=2)}\")\nif crew_output.pydantic:\n    print(f\"Pydantic Output: {crew_output.pydantic}\")\nprint(f\"Tasks Output: {crew_output.tasks_output}\")\nprint(f\"Token Usage: {crew_output.token_usage}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T14:53:57.155356Z","iopub.status.idle":"2025-03-18T14:53:57.155705Z","shell.execute_reply":"2025-03-18T14:53:57.155566Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Flow (1) for only Domain and code -> Test","metadata":{}},{"cell_type":"code","source":"# Define the flow state\nclass LinuxFlowState(BaseModel):\n    prompt: str = \"\"\n    domains: list = None\n    domain_analysis: DomainAnalysis = None\n    command_output: CommandOutput = None\n\n\nclass LinuxFlow(Flow[LinuxFlowState]):\n    \"\"\"Flow to execute the agents sequentially\"\"\"\n\n    @start()\n    def get_user_input(self):\n        \"\"\"Get prompt from the user\"\"\"\n        self.state.prompt = \"list all files in my directory starts with 'a'\"\n        self.state.domains = domains_above\n        return self.state\n\n    @listen(get_user_input)\n    def execute_domain_agent(self, state: LinuxFlowState):\n        \"\"\"Execute the domain agent\"\"\"\n        \n        domain_response = domain_agent.execute_task(domain_task,\n                context=json.dumps({\n                \"prompt\": state.prompt,\n                \"domains\": \", \".join(state.domains)\n                }))\n        state.domain_analysis = DomainAnalysis.parse_raw(domain_response)\n        \n        print(f\"\\nDomains identified: {state.domain_analysis.domains}\")\n        print(f\"Confidence: {state.domain_analysis.confidence}\")\n        \n        return state\n\n    @listen(execute_domain_agent)\n    def execute_command_agent(self, state: LinuxFlowState):\n        \"\"\"Execute the command agent\"\"\"\n        if not state.domain_analysis:\n            print(\"No domain analysis found. Exiting.\")\n            return state\n        \n        domains_str = \", \".join(state.domain_analysis.domains)\n        \n        command_response = command_agent.execute_task(\n            command_task,\n            context=json.dumps({\n                \"prompt\": state.prompt,\n                \"domains\": \", \".join(state.domain_analysis.domains)\n            }),\n            tools=None\n        )\n\n        state.command_output = CommandOutput.parse_raw(command_response)\n        \n        print(f\"\\nGenerated Command: {state.command_output.main_command}\")\n        \n        return state","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T14:53:57.156718Z","iopub.status.idle":"2025-03-18T14:53:57.157085Z","shell.execute_reply":"2025-03-18T14:53:57.156933Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## RUN Flow 1","metadata":{}},{"cell_type":"code","source":"# async def kickoff():\n#     \"\"\"Run the Linux flow\"\"\"\n#     await LinuxFlow().kickoff_async()\n\n\n# # Run the flow asynchronously for compatibility with Jupyter/Colab\n# import asyncio\n# await kickoff()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T14:53:57.157996Z","iopub.status.idle":"2025-03-18T14:53:57.158392Z","shell.execute_reply":"2025-03-18T14:53:57.158245Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Plot flow 1","metadata":{}},{"cell_type":"code","source":"LinuxFlow().plot(\"guide_creator_flow\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T14:53:57.159141Z","iopub.status.idle":"2025-03-18T14:53:57.159552Z","shell.execute_reply":"2025-03-18T14:53:57.159411Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Flow 2 Code","metadata":{}},{"cell_type":"code","source":"# Define the flow state\nclass LinuxFlowState(BaseModel):\n    prompt: str = \"\"\n    domains: list = None\n    domain_analysis: DomainAnalysis = None\n    command_output: CommandOutput = None\n    context_data: dict = None\n\nclass LinuxFlow(Flow[LinuxFlowState]):\n    \"\"\"Flow to execute the agents sequentially\"\"\"\n\n    @start()\n    def get_user_input(self):\n        \"\"\"Get prompt from the user\"\"\"\n        self.state.prompt = \"what is my passward?\"\n        self.state.domains = domains_above\n        return self.state\n\n    @listen(get_user_input)\n    def execute_domain_agent(self, state: LinuxFlowState):\n        \"\"\"Execute the domain agent\"\"\"\n        domain_response = domain_agent.execute_task(domain_task,\n                context=json.dumps({\n                \"prompt\": state.prompt,\n                \"domains\": \", \".join(state.domains)\n                }))\n        state.domain_analysis = DomainAnalysis.parse_raw(domain_response)\n        print(f\"\\nDomains identified: {state.domain_analysis.domains}\")\n        return state\n\n    @listen(execute_domain_agent)\n    def retrieve_context(self, state: LinuxFlowState):\n        \"\"\"Retrieve technical context using RAG\"\"\"\n        if not state.domain_analysis:\n            return state\n            \n        state.context_data = {}\n        for domain in state.domain_analysis.domains:\n            context_response = context_agent.execute_task(\n                context_retrieval_task,\n                context=json.dumps({\n                    \"domain\": domain,\n                    \"prompt\": state.prompt\n                })\n            )\n            state.context_data[domain] = context_response\n            \n        print(\"\\nRetrieved Context:\")\n        for domain, context in state.context_data.items():\n            print(f\"- {domain}: {context[:50]}...\")\n        return state\n\n    @listen(retrieve_context)\n    def execute_command_agent(self, state: LinuxFlowState):\n        \"\"\"Generate command with domain context\"\"\"\n        if not state.domain_analysis or not state.context_data:\n            print(\"Missing domain analysis or context. Exiting.\")\n            return state\n            \n        command_response = command_agent.execute_task(\n            command_task,\n            context=json.dumps({\n                \"prompt\": state.prompt,\n                \"domains\": \", \".join(state.domain_analysis.domains),\n                \"context\": json.dumps(state.context_data)\n            })\n        )\n        \n        state.command_output = CommandOutput.parse_raw(command_response)\n        print(f\"\\nFinal Command: {state.command_output.main_command}\")\n        return state","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T14:53:57.162000Z","iopub.status.idle":"2025-03-18T14:53:57.162458Z","shell.execute_reply":"2025-03-18T14:53:57.162293Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"async def kickoff():\n    \"\"\"Run the Linux flow\"\"\"\n    await LinuxFlow().kickoff_async()\n\n\n# Run the flow asynchronously for compatibility with Jupyter/Colab\nimport asyncio\nawait kickoff()\n\n\nLinuxFlow().plot(\"guide_creator_flow\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T14:53:57.163374Z","iopub.status.idle":"2025-03-18T14:53:57.163748Z","shell.execute_reply":"2025-03-18T14:53:57.163594Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Make a fake flow 2 -> what we can do at the end","metadata":{}},{"cell_type":"code","source":"from crewai.flow.flow import Flow, listen, router, start, or_\nfrom pydantic import BaseModel\n\n\n# Define Flow State\nclass CustomFlowState(BaseModel):\n    prompt: str = \"\"\n    domain_output: dict = None\n    context_data: dict = None\n    qa_data: dict = None\n    code_data: dict = None\n\n\nclass flow_fake(Flow[CustomFlowState]):\n\n    @start()\n    def user_input(self):\n        \"\"\"User provides a prompt to initiate the flow.\"\"\"\n        self.state.prompt = \"User's input prompt here\" \n        return \"domain_block\"\n\n    @listen(user_input)\n    def domain_block(self):\n        \"\"\"Processes the user prompt for domain analysis.\"\"\"\n        self.state.domain_output = {\"domains\": [\"example_domain\"]}\n        return \"router\"\n\n    @router(or_(domain_block,\"context_retriever\",\"qa_block\",\"code_block\"))\n    def router(self):\n        \"\"\"Routes the flow based on state information.\"\"\"\n        # Example routing logic: Cycle through context, QA, and Code blocks\n        if not self.state.context_data:\n            return \"context_retriever_from_route\"\n        elif not self.state.qa_data:\n            return \"qa_block_from_route\"\n        elif not self.state.code_data:\n            return \"code_block_from_route\"\n        else:\n            return \"finish_from_route\"\n\n    # Context Retriever Bidirectional Flow\n    @listen(or_(\"context_retriever_from_route\",\"rag_block\"))\n    def context_retriever(self):\n        \"\"\"Handles retrieving context and interacting with RAG.\"\"\"\n        self.state.context_data = {\"retrieved_info\": \"sample context data\"}\n        return \"router\"  # Return to router\n\n    @listen(context_retriever)\n    def rag_block(self):\n        \"\"\"Handles the RAG, Queue, and Stuff.\"\"\"\n        # Simulate RAG processing\n        return \"context_retriever\"\n\n    # QA Bidirectional Flow\n    @listen(\"qa_block_from_route\")\n    def qa_block(self):\n        \"\"\"Handles question-answering logic.\"\"\"\n        self.state.qa_data = {\"qa_response\": \"sample QA response\"}\n        return \"router\"  # Return to router\n\n    # Code Bidirectional Flow\n    @listen(\"code_block_from_route\")\n    def code_block(self):\n        \"\"\"Handles code generation tasks.\"\"\"\n        self.state.code_data = {\"code_output\": \"sample generated code\"}\n        return \"router\"  # Return to router\n\n    # Flow Completion\n    @listen(\"finish_from_route\")\n    def finish(self):\n        \"\"\"Completes the flow after all tasks are processed.\"\"\"\n\n    @listen(finish)\n    def end(self):\n        \"\"\"Finish the prompt and return what's inside\"\"\"\n        return self.state\n\n\nflow = flow_fake()\nflow.plot(\"flow_2\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T14:53:57.164562Z","iopub.status.idle":"2025-03-18T14:53:57.164909Z","shell.execute_reply":"2025-03-18T14:53:57.164770Z"}},"outputs":[],"execution_count":null}]}